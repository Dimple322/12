# agent/vector/indexer.py (–ª–æ–∫–∞–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π + –±–µ–∑–æ–ø–∞—Å–Ω—ã–π fallback)
import os
import torch
from pathlib import Path
from typing import List, Dict

import chromadb
from sentence_transformers import SentenceTransformer

# –ü—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ (–≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏)
MODEL_DIR = Path(__file__).resolve().parent.parent.parent / "models" / "sentence"

# –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –ª–æ–∫–∞–ª—å–Ω–æ; –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ ‚Äî –≤—ã–¥–∞—ë–º –ø–æ–Ω—è—Ç–Ω—ã–π warning –∏ –æ—Ç–∫–ª—é—á–∞–µ–º —ç–Ω–∫–æ–¥–µ—Ä
ENCODER = None
try:
    if MODEL_DIR.exists() and any(MODEL_DIR.iterdir()):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –ø–∞–ø–∫—É –º–æ–¥–µ–ª–∏
        ENCODER = SentenceTransformer(str(MODEL_DIR))
        print(f"[INDEXER] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å SentenceTransformer –∏–∑: {MODEL_DIR}")
    else:
        # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ –∏–º–µ–Ω–∏ (–Ω–æ —ç—Ç–æ –ø–æ—Ç—Ä–µ–±—É–µ—Ç –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞)
        print("[INDEXER] –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ –∏–º–µ–Ω–∏ (–Ω—É–∂–Ω–æ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç).")
        ENCODER = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
except Exception as e:
    print(f"[INDEXER WARNING] –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å SentenceTransformer: {e}")
    ENCODER = None

def index_documents(texts: List[str], metas: List[Dict[str, str]]):
    if not texts:
        print("‚ö†Ô∏è –ù–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
        return

    db_path = Path(__file__).resolve().parent.parent.parent / "generated" / "chroma_db"
    db_path.mkdir(parents=True, exist_ok=True)

    client = chromadb.PersistentClient(path=str(db_path))
    coll = client.get_or_create_collection("docs", metadata={"hnsw:space": "cosine"})

    if ENCODER is None:
        print("‚ö†Ô∏è ENCODER –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω, –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–∞.")
        return

    print(f"üß† –ö–æ–¥–∏—Ä—É–µ–º {len(texts)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...")
    embeddings = ENCODER.encode(texts, normalize_embeddings=True, show_progress_bar=True).tolist()

    ids = [f"id_{i}" for i in range(len(texts))]
    coll.add(documents=texts, embeddings=embeddings, metadatas=metas, ids=ids)
    print(f"‚úÖ –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ {len(texts)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ (–ª–æ–∫–∞–ª—å–Ω–æ)")